#!/usr/bin/python3
# -*- coding: utf-8 -*-
"""
https://github.com/jingyaogong/minimind/blob/master/eval_llm.py
"""
import argparse
import time

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer

from project_settings import project_path


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--pretrained_model_name_or_path",
        # default="jingyaogong/MiniMind2",
        default=(project_path / "pretrained_models/MiniMind2"),
        type=str
    )

    parser.add_argument(
        "--max_new_tokens",
        default=8192, # 8192, 128
        type=int, help="æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼ˆæ³¨æ„ï¼šå¹¶éæ¨¡å‹å®é™…é•¿æ–‡æœ¬èƒ½åŠ›ï¼‰"
    )
    parser.add_argument("--top_p", default=0.85, type=float, help="nucleusé‡‡æ ·é˜ˆå€¼ï¼ˆ0-1ï¼‰")
    parser.add_argument("--temperature", default=0.85, type=float, help="ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶éšæœºæ€§ï¼ˆ0-1ï¼Œè¶Šå¤§è¶Šéšæœºï¼‰")

    parser.add_argument(
        "--show_speed",
        default=1,  # 1, 0
        type=int, help="æ˜¾ç¤ºdecodeé€Ÿåº¦ï¼ˆtokens/sï¼‰"
    )

    args = parser.parse_args()
    return args


def main():
    args = get_args()

    if torch.cuda.is_available():
        device = "cuda"
    elif torch.backends.mps.is_available():
        # device = "mps"
        device = "cpu"
    else:
        device = "cpu"
    print(f"device: {device}")

    tokenizer = AutoTokenizer.from_pretrained(args.pretrained_model_name_or_path)
    model = AutoModelForCausalLM.from_pretrained(args.pretrained_model_name_or_path)
    model = model.eval().to(device)
    # print(tokenizer)
    # print(model)

    prompts = [
        "ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ",
        "ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„",
        "è¯·ç”¨Pythonå†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°",
        'è§£é‡Šä¸€ä¸‹"å…‰åˆä½œç”¨"çš„åŸºæœ¬è¿‡ç¨‹',
        "å¦‚æœæ˜å¤©ä¸‹é›¨ï¼Œæˆ‘åº”è¯¥å¦‚ä½•å‡ºé—¨",
        "æ¯”è¾ƒä¸€ä¸‹çŒ«å’Œç‹—ä½œä¸ºå® ç‰©çš„ä¼˜ç¼ºç‚¹",
        "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
        "æ¨èä¸€äº›ä¸­å›½çš„ç¾é£Ÿ"
    ]
    input_mode = int(input("[0] è‡ªåŠ¨æµ‹è¯•\n[1] æ‰‹åŠ¨è¾“å…¥\n"))

    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    # conversation = list()
    conversation = [
        {"role": "system", "content": "You are a helpful assistant"}
    ]
    while True:
        if input_mode == 0:
            if len(prompts) == 0:
                break
            user_input = prompts.pop(0)
            print(f"ğŸ’¬: {user_input}")
        else:
            user_input = input("ğŸ’¬: ")
            user_input = str(user_input).strip()
        conversation.append({"role": "user", "content": user_input})
        inputs = tokenizer.apply_chat_template(
            conversation=conversation,
            tokenize=False,
            add_generation_prompt=True
        )
        inputs = tokenizer.__call__(
            inputs,
            return_tensors="pt",
            truncation=True
        )
        inputs = inputs.to(device)
        # print(inputs)

        print("ğŸ¤–: ", end="")
        st = time.time()
        generated_ids = model.generate(
            inputs=inputs["input_ids"], attention_mask=inputs["attention_mask"],
            max_new_tokens=args.max_new_tokens, do_sample=True, streamer=streamer,
            pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id,
            top_p=args.top_p, temperature=args.temperature, repetition_penalty=1.0,
        )
        response = tokenizer.decode(generated_ids[0][len(inputs["input_ids"][0]):], skip_special_tokens=True)
        conversation.append({"role": "assistant", "content": response})
        gen_tokens = len(generated_ids[0]) - len(inputs["input_ids"][0])
        print(f"\n[Speed]: {gen_tokens / (time.time() - st):.2f} tokens/s\n\n") if args.show_speed else print("\n\n")

    return


if __name__ == "__main__":
    main()
